[["index.html", "TwitterAPI Chapter 1 Introduction", " TwitterAPI Sung Jun Won(sw3049), Hyo Won (Elin) Kim(hk3175), Alex Kita(ak4729) 2021-11-29 Chapter 1 Introduction Different people have varied opinions about the coronavirus on Twitter. Some are overly anxious about the impact of the virus while others are overly optimistic. We wanted to explore how different people react to the same news by analyzing the sentiments of their tweets. We focused on the groups of people who have significant impacts on the public opinion, namely celebrities (top 100 accounts), medical associations, politicians, news media agencies. book URL: https://twittersentimentanalyze.github.io/twittersentiment/ This repo was initially generated from a bookdown template available here: https://github.com/jtr13/EDAVtemplate "],["data-sources.html", "Chapter 2 Data sources", " Chapter 2 Data sources "],["data-transformation.html", "Chapter 3 Data transformation", " Chapter 3 Data transformation "],["missing-values.html", "Chapter 4 Missing values", " Chapter 4 Missing values library(tidyverse) library(patchwork) library(ggnewscale) library(rtweet) library(sjmisc) library(readr) Here is the missing value plot function we defined in Problem Set 4. plot_missing &lt;- function(data, percent) { colnames(data) &lt;- abbreviate(colnames(data), minlength = 3) mp &lt;- data.frame(is.na(data)) %&gt;% group_by_all() %&gt;% count(name = &quot;count&quot;, sort = TRUE) %&gt;% ungroup() missing_patterns &lt;- mp %&gt;% select(c(1:ncol(mp)-1)) sorted_num_missing &lt;- sort(colSums(is.na(data)), decreasing = TRUE, index.return=TRUE) level &lt;- colnames(missing_patterns[sorted_num_missing$ix]) mid &lt;- level[ceiling(length(level)/2)] # get name of var located at the center of graph to put &quot;complete case&quot; tidy_missing_patterns &lt;- missing_patterns %&gt;% add_column(complete = if_else(rowSums(missing_patterns)==0, TRUE, FALSE)) %&gt;% rownames_to_column(&quot;id&quot;) %&gt;% gather(key, value, c(-id,-complete)) df &lt;- sorted_num_missing$x top_count &lt;- data.frame( name=names(df) , value=df) top_percent &lt;- data.frame( name=names(df) , value=df*100/nrow(data)) cmp &lt;- missing_patterns %&gt;% add_column(complete = if_else(rowSums(missing_patterns)==0, TRUE, FALSE)) side_count &lt;- data.frame( name=as.factor(seq.int(1:nrow(missing_patterns))), value=mp$count, complete=cmp$complete ) side_percent &lt;- data.frame( name=as.factor(seq.int(1:nrow(missing_patterns))), value=mp$count*100/nrow(data), complete=cmp$complete ) if (percent) { p1 &lt;- ggplot(top_percent, aes(x=fct_relevel(name, level), y=value)) + geom_bar(stat = &quot;identity&quot;, alpha=0.8, fill=&quot;#97b7f3&quot;) + ylim(0, 100) + ylab(&quot;% rows missing:&quot;) + theme(axis.title.x=element_blank()) + ggtitle(&quot;Missing value patterns&quot;) p2 &lt;-ggplot(side_percent, aes(x=fct_rev(name), y=value, fill=complete)) + geom_bar(stat = &quot;identity&quot;, alpha=0.8) + scale_fill_manual(values = c(&quot;#97b7f3&quot;, &quot;#6396ec&quot;)) + coord_flip() + ylab(&quot;% row&quot;) + ylim(0, 100) + theme(axis.title.y=element_blank()) + theme(legend.position = &quot;none&quot;) } else { p1 &lt;- ggplot(top_count, aes(x=fct_relevel(name, level), y=value)) + geom_bar(stat = &quot;identity&quot;, alpha=0.8, fill=&quot;#97b7f3&quot;) + ylab(&quot;num rows missing:&quot;) + theme(axis.title.x=element_blank()) + ggtitle(&quot;Missing value patterns&quot;) p2 &lt;- ggplot(side_count, aes(x=fct_rev(name), y=value, fill=complete)) + geom_bar(stat = &quot;identity&quot;, alpha=0.8) + scale_fill_manual(values = c(&quot;#97b7f3&quot;, &quot;#6396ec&quot;)) + coord_flip() + ylab(&quot;row count&quot;) + theme(axis.title.y=element_blank()) + theme(legend.position = &quot;none&quot;) } tidy_missing_patterns$id &lt;- as.factor(as.integer(tidy_missing_patterns$id)) p3 &lt;-ggplot(tidy_missing_patterns, aes(x = fct_relevel(key, level), y = fct_rev(id), fill = value)) + geom_tile(color = &quot;white&quot;, aes(alpha=complete)) + geom_text(aes(label = ifelse(complete == TRUE &amp; key==mid, &quot;complete cases&quot;, &quot;&quot;))) + scale_fill_manual(values = c(&quot;#b3b3b3&quot;, &quot;#9370da&quot;)) + scale_alpha_manual(values = c(0.7, 1.0)) + xlab(&quot;variable&quot;) + ylab(&quot;missing pattern&quot;) + theme_bw() + theme(legend.position = &quot;none&quot;) p1 + plot_spacer() + p3 + p2 + plot_layout(widths = c(3,1), heights = unit(c(2,1), c(&#39;cm&#39;,&#39;null&#39;))) } Our dataset contains top 50 most followed twitter users. We are mainly concerned with the tweets regarding COVID-19, so we set various tags to filter out the tweets containing at least one of the tags. names &lt;- c(&#39;BarackObama&#39;, &#39;justinbieber&#39;, &#39;ArianaGrande&#39;, &#39;ladygaga&#39;, &#39;YouTube&#39;, &#39;KimKardashian&#39;, &#39;elonmusk&#39;, &#39;Twitter&#39;, &#39;BillGates&#39;, &#39;CNN&#39;, &#39;ddlovato&#39;, &#39;nytimes&#39;, &#39;NASA&#39;, &#39;BBCBreaking&#39;, &#39;KylieJenner&#39;, &#39;wizkhalifa&#39;, &#39;LilTunechi&#39;, &#39;NBA&#39;, &#39;POTUS&#39;, &#39;Pink&#39;, &#39;JoeBiden&#39;, &#39;NFL&#39;, &#39;AmitShah&#39;, &#39;WhiteHouse&#39;, &#39;danieltosh&#39;, &#39;AnushkaSharma&#39;, &#39;davidguetta&#39;, &#39;SnoopDogg&#39;, &#39;WSJ&#39;, &#39;KamalaHarris&#39;, &#39;SpaceX&#39;, &#39;VancityReynolds&#39;, &#39;Forbes&#39;, &#39;ABC&#39;, &#39;RobertDowneyJr&#39;, &#39;FLOTUS&#39;, &#39;TheRock&#39;, &#39;AP&#39;, &#39;RyanSeacrest&#39;, &#39;UN&#39;, &#39;BigSean&#39;, &#39;johnlegend&#39;, &#39;netflix&#39;, &#39;FortniteGame&#39;, &#39;JohnCena&#39;, &#39;cnni&#39;, &#39;AOC&#39;, &#39;Marvel&#39;, &#39;enews&#39;) tag &lt;- c(&#39;corona&#39;, &#39;#corona&#39;, &#39;coronavirus&#39;, &#39;#coronavirus&#39;, &#39;covid&#39;, &#39;#covid&#39;, &#39;covid19&#39;, &#39;#covid19&#39;, &#39;covid-19&#39;, &#39;#covid-19&#39;, &#39;sarscov2&#39;, &#39;#sarscov2&#39;, &#39;sars cov2&#39;, &#39;sars cov 2&#39;, &#39;covid_19&#39;, &#39;#covid_19&#39;, &#39;#ncov&#39;, &#39;ncov&#39;, &#39;#ncov2019&#39;, &#39;ncov2019&#39;, &#39;2019-ncov&#39;, &#39;#2019-ncov&#39;, &#39;pandemic&#39;, &#39;#pandemic #2019ncov&#39;, &#39;2019ncov&#39;, &#39;quarantine&#39;, &#39;#quarantine&#39;, &#39;flatten the curve&#39;, &#39;flattening the curve&#39;, &#39;#flatteningthecurve&#39;, &#39;#flattenthecurve&#39;, &#39;hand sanitizer&#39;, &#39;#handsanitizer&#39;, &#39;#lockdown&#39;, &#39;lockdown&#39;, &#39;social distancing&#39;, &#39;#socialdistancing&#39;, &#39;work from home&#39;, &#39;#workfromhome&#39;, &#39;working from home&#39;, &#39;#workingfromhome&#39;, &#39;ppe&#39;, &#39;n95&#39;, &#39;#ppe&#39;, &#39;#n95&#39;, &#39;#covidiots&#39;, &#39;covidiots&#39;, &#39;herd immunity&#39;, &#39;#herdimmunity&#39;, &#39;pneumonia&#39;, &#39;#pneumonia&#39;, &#39;chinese virus&#39;, &#39;#chinesevirus&#39;, &#39;wuhan virus&#39;, &#39;#wuhanvirus&#39;, &#39;kung flu&#39;, &#39;#kungflu&#39;, &#39;wearamask&#39;, &#39;#wearamask&#39;, &#39;wear a mask&#39;, &#39;vaccine&#39;, &#39;vaccines&#39;, &#39;#vaccine&#39;, &#39;#vaccines&#39;, &#39;corona vaccine&#39;, &#39;corona vaccines&#39;, &#39;#coronavaccine&#39;, &#39;#coronavaccines&#39;, &#39;face shield&#39;, &#39;#faceshield&#39;, &#39;face shields&#39;, &#39;#faceshields&#39;, &#39;health worker&#39;, &#39;#healthworker&#39;, &#39;health workers&#39;, &#39;#healthworkers&#39;, &#39;#stayhomestaysafe&#39;, &#39;#coronaupdate&#39;, &#39;#frontlineheroes&#39;, &#39;#coronawarriors&#39;, &#39;#homeschool&#39;, &#39;#homeschooling&#39;, &#39;#hometasking&#39;, &#39;#masks4all&#39;, &#39;#wfh&#39;, &#39;wash ur hands&#39;, &#39;wash your hands&#39;, &#39;#washurhands&#39;, &#39;#washyourhands&#39;, &#39;#stayathome&#39;, &#39;#stayhome&#39;, &#39;#selfisolating&#39;, &#39;self isolating&#39;) We collected 200 most recent tweets from top 50 users and chose the tweets that have at least one of the tags in them. write_df &lt;- data.frame() for (name in names){ new &lt;- get_timelines(name, n=200) write_df &lt;- rbind(new, write_df) } write_csv(write_df, &quot;./resources/data.csv&quot;) Get the tweet data from the csv file. df &lt;- read_csv(&#39;./resources/data.csv&#39;) We define the function that cleans the tweet data. Reference: https://stackoverflow.com/questions/31348453/how-do-i-clean-twitter-data-in-r clean_tweets &lt;- function(x) { x %&gt;% # Remove URLs str_remove_all(&quot; ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)&quot;) %&gt;% # Remove mentions e.g. &quot;@my_account&quot; str_remove_all(&quot;@[[:alnum:]_]{4,}&quot;) %&gt;% # Replace &quot;&amp;&quot; character reference with &quot;and&quot; str_replace_all(&quot;&amp;amp;&quot;, &quot;and&quot;) %&gt;% # Remove punctuation, using a standard character class str_remove_all(&quot;[[:punct:]]&quot;) %&gt;% # Remove &quot;RT: &quot; from beginning of retweets str_remove_all(&quot;^RT:? &quot;) %&gt;% # Replace any newline characters with a space str_replace_all(&quot;\\\\\\n&quot;, &quot; &quot;) %&gt;% # Make everything lowercase str_to_lower() %&gt;% # Remove any trailing whitespace around the text str_trim(&quot;both&quot;) %&gt;% # remove unnecessary space str_replace_all(&quot; &quot;,&quot; &quot;) } Clean the tweet data and extract the tweets that contains COVID related tags. output &lt;- c() cleaned_text &lt;- clean_tweets(df$text) for (words in str_split(cleaned_text, &quot; &quot;)) { f &lt;- if_else(any(words %in% tag), 1, 0) output &lt;- append(output, f) } df$output &lt;- output df %&gt;% filter(output == 1) -&gt; df2 After filtering, there are total of 403 rows (tweets). nrow(df2) ## [1] 403 Then we checked how many missing values each column has. df2 %&gt;% summarise_all(funs(sum(is.na(.)))) ## # A tibble: 1 x 91 ## user_id status_id created_at screen_name text source display_text_width ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 0 0 0 0 0 0 0 ## # ... with 84 more variables: reply_to_status_id &lt;int&gt;, reply_to_user_id &lt;int&gt;, ## # reply_to_screen_name &lt;int&gt;, is_quote &lt;int&gt;, is_retweet &lt;int&gt;, ## # favorite_count &lt;int&gt;, retweet_count &lt;int&gt;, quote_count &lt;int&gt;, ## # reply_count &lt;int&gt;, hashtags &lt;int&gt;, symbols &lt;int&gt;, urls_url &lt;int&gt;, ## # urls_t.co &lt;int&gt;, urls_expanded_url &lt;int&gt;, media_url &lt;int&gt;, ## # media_t.co &lt;int&gt;, media_expanded_url &lt;int&gt;, media_type &lt;int&gt;, ## # ext_media_url &lt;int&gt;, ext_media_t.co &lt;int&gt;, ... plot_missing(df2, percent = TRUE) plot_missing(df2, percent = FALSE) It is hard to draw any significant conclusion from columns that have more than 95% of entries with missing values. Thus, we decided to remove all columns that have NA in more than 95% of entries. df3 &lt;- df2 %&gt;% select(which(colMeans(is.na(.)) &lt;= 0.95)) df3 ## # A tibble: 403 x 48 ## user_id status_id created_at screen_name text source ## &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2883841 1.46e18 2021-11-17 19:17:35 enews &quot;#DWTS judge Der~ Social~ ## 2 2883841 1.46e18 2021-11-16 21:58:32 enews &quot;9-1-1 Star Rock~ Social~ ## 3 2883841 1.46e18 2021-11-16 17:48:51 enews &quot;Dancing With th~ Social~ ## 4 2883841 1.46e18 2021-11-16 15:24:06 enews &quot;Miles Teller Se~ Social~ ## 5 138203134 1.46e18 2021-11-04 15:57:56 AOC &quot;You can see thi~ Twitte~ ## 6 138203134 1.45e18 2021-10-15 04:18:53 AOC &quot;And by the way,~ Twitte~ ## 7 138203134 1.45e18 2021-10-14 14:57:46 AOC &quot;#Striketober co~ Twitte~ ## 8 138203134 1.45e18 2021-10-13 22:40:09 AOC &quot;Thank you all f~ Twitte~ ## 9 2097571 1.46e18 2021-11-18 20:46:03 cnni &quot;Richard Ayvazya~ Social~ ## 10 2097571 1.46e18 2021-11-18 19:45:06 cnni &quot;Winter is almos~ Social~ ## # ... with 393 more rows, and 42 more variables: display_text_width &lt;dbl&gt;, ## # reply_to_user_id &lt;dbl&gt;, reply_to_screen_name &lt;chr&gt;, is_quote &lt;lgl&gt;, ## # is_retweet &lt;lgl&gt;, favorite_count &lt;dbl&gt;, retweet_count &lt;dbl&gt;, lang &lt;chr&gt;, ## # retweet_status_id &lt;dbl&gt;, retweet_text &lt;chr&gt;, retweet_created_at &lt;dttm&gt;, ## # retweet_source &lt;chr&gt;, retweet_favorite_count &lt;dbl&gt;, ## # retweet_retweet_count &lt;dbl&gt;, retweet_user_id &lt;dbl&gt;, ## # retweet_screen_name &lt;chr&gt;, retweet_name &lt;chr&gt;, ... df3 %&gt;% summarise_all(funs(sum(is.na(.)))) ## # A tibble: 1 x 48 ## user_id status_id created_at screen_name text source display_text_width ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 0 0 0 0 0 0 0 ## # ... with 41 more variables: reply_to_user_id &lt;int&gt;, ## # reply_to_screen_name &lt;int&gt;, is_quote &lt;int&gt;, is_retweet &lt;int&gt;, ## # favorite_count &lt;int&gt;, retweet_count &lt;int&gt;, lang &lt;int&gt;, ## # retweet_status_id &lt;int&gt;, retweet_text &lt;int&gt;, retweet_created_at &lt;int&gt;, ## # retweet_source &lt;int&gt;, retweet_favorite_count &lt;int&gt;, ## # retweet_retweet_count &lt;int&gt;, retweet_user_id &lt;int&gt;, ## # retweet_screen_name &lt;int&gt;, retweet_name &lt;int&gt;, ... plot_missing(df3, percent = TRUE) plot_missing(df3, percent = FALSE) We can observe that columns [retweet_status_id, retweet_text, retweet_created_at, retweet_source, retweet_favorite_count, retweet_retweet_count, retweet_user_id, retweet_screen_name, retweet_name, retweet_followers_count, retweet_friends_count, retweet_statuses_count, retweet_verified] all have 365 missing values, meaning they all have correlations to each other. They have equal number of missing values and there are 365 out of 405 tweets that have those column values missing, so we can assume that most twitter users dont retweet the tweets regarding COVID. Also, columns [reply_to_user_id, reply_to_screen_name] have 384 values missing. These two columns are correlated to each other. Having 384 out of 405 entries missing indicates that most of the tweets regarding COVID are original posts, not replies to other posts. Column location has 78 missing values, which is approximately 25% of the 405 total entries. It seems that majority of the tweets regarding COVID have locations indicated, but it is at users discretion. Moreover, column profile_background_url has 48 missing values, which is approximately 12% of 405. The low proportion of missing values indicates that most users have profile background url posted, but it is also up to users choice. There are still many columns that are not very significant for our final projects purpose, even some columns that have no missing values. Thus, we trimmed down the number of columns further. Columns such as [source, display_text_width, reply_to_user_id, reply_to_screen_name, is_quote, is_retweet, retweet_status_id, retweet_text, retweet_created_at, retweet_source, retweet_favorite_count, retweet_retweet_count, retweet_user_id, retweet_screen_name, retweet_name, retweet_followers_count, retweet_friends_count, retweet_statuses_count, retweet_location, retweet_description, retweet_verified, status_url, url, protected, profile_url, profile_expanded_url, profile_banner_url, profile_background_url, profile_image_url, output] are all irrelevant or very insignificant in telling users activity and tweets regarding COVID, which is what we are mainly concerned with. Thus, we removed all those columns for the sake of simplicity and cleanliness. cols_to_remove &lt;- c(&#39;source&#39;, &#39;display_text_width&#39;, &#39;reply_to_user_id&#39;, &#39;reply_to_screen_name&#39;, &#39;is_quote&#39;, &#39;is_retweet&#39;, &#39;retweet_status_id&#39;, &#39;retweet_text&#39;, &#39;retweet_created_at&#39;, &#39;retweet_source&#39;, &#39;retweet_favorite_count&#39;, &#39;retweet_retweet_count&#39;, &#39;retweet_user_id&#39;, &#39;retweet_screen_name&#39;, &#39;retweet_name&#39;, &#39;retweet_followers_count&#39;, &#39;retweet_friends_count&#39;, &#39;retweet_statuses_count&#39;, &#39;retweet_location&#39;, &#39;retweet_description&#39;, &#39;retweet_verified&#39;, &#39;status_url&#39;, &#39;url&#39;, &#39;protected&#39;, &#39;profile_url&#39;, &#39;profile_expanded_url&#39;, &#39;profile_banner_url&#39;, &#39;profile_background_url&#39;, &#39;profile_image_url&#39;, &#39;output&#39;) df4 &lt;- select(df3, -cols_to_remove) df4 ## # A tibble: 403 x 18 ## user_id status_id created_at screen_name text favorite_count ## &lt;dbl&gt; &lt;dbl&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 2883841 1.46e18 2021-11-17 19:17:35 enews &quot;#DWTS ju~ 46 ## 2 2883841 1.46e18 2021-11-16 21:58:32 enews &quot;9-1-1 St~ 82 ## 3 2883841 1.46e18 2021-11-16 17:48:51 enews &quot;Dancing ~ 73 ## 4 2883841 1.46e18 2021-11-16 15:24:06 enews &quot;Miles Te~ 82 ## 5 138203134 1.46e18 2021-11-04 15:57:56 AOC &quot;You can ~ 2123 ## 6 138203134 1.45e18 2021-10-15 04:18:53 AOC &quot;And by t~ 18504 ## 7 138203134 1.45e18 2021-10-14 14:57:46 AOC &quot;#Striket~ 14418 ## 8 138203134 1.45e18 2021-10-13 22:40:09 AOC &quot;Thank yo~ 83780 ## 9 2097571 1.46e18 2021-11-18 20:46:03 cnni &quot;Richard ~ 33 ## 10 2097571 1.46e18 2021-11-18 19:45:06 cnni &quot;Winter i~ 63 ## # ... with 393 more rows, and 12 more variables: retweet_count &lt;dbl&gt;, ## # lang &lt;chr&gt;, name &lt;chr&gt;, location &lt;chr&gt;, description &lt;chr&gt;, ## # followers_count &lt;dbl&gt;, friends_count &lt;dbl&gt;, listed_count &lt;dbl&gt;, ## # statuses_count &lt;dbl&gt;, favourites_count &lt;dbl&gt;, account_created_at &lt;dttm&gt;, ## # verified &lt;lgl&gt; plot_missing(df4, percent = TRUE) plot_missing(df4, percent = FALSE) We can see that out of 18 columns, only location column has some missing values remaining. We would have to impute the missing values or make a separate column for missing value indicator if we deem this column to be important for our future analysis. We used 200 tweets of top 50 users for creating this dataset, so the size of the dataset is small compared to what we will have for the final version of our project. When we use more tweets on more users, we may potentially have some more important observations. "],["results.html", "Chapter 5 Results 5.1 What were the sentimental reactions of different groups to Covid-19 news? 5.2 Deep dive into reactions of politicians and news media: 5.3 Deep dive into correlations of key metrics and sentiments:", " Chapter 5 Results 5.1 What were the sentimental reactions of different groups to Covid-19 news? Ratio of covid-related post / total post 5.2 Deep dive into reactions of politicians and news media: Relationship between political stance and reactions 5.3 Deep dive into correlations of key metrics and sentiments: Do retweet/likes/reply have any correlation with certain sentiments? "],["interactive-component.html", "Chapter 6 Interactive component", " Chapter 6 Interactive component "],["conclusion.html", "Chapter 7 Conclusion 7.1 R Markdown", " Chapter 7 Conclusion 7.1 R Markdown This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see http://rmarkdown.rstudio.com. When you click the Knit button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this: library(rtweet) library(sjmisc) library(dplyr) tag &lt;- c(&#39;corona&#39;, &#39;#corona&#39;, &#39;coronavirus&#39;, &#39;#coronavirus&#39;, &#39;covid&#39;, &#39;#covid&#39;, &#39;covid19&#39;, &#39;#covid19&#39;, &#39;covid-19&#39;, &#39;#covid-19&#39;, &#39;sarscov2&#39;, &#39;#sarscov2&#39;, &#39;sars cov2&#39;, &#39;sars cov 2&#39;, &#39;covid_19&#39;, &#39;#covid_19&#39;, &#39;#ncov&#39;, &#39;ncov&#39;, &#39;#ncov2019&#39;, &#39;ncov2019&#39;, &#39;2019-ncov&#39;, &#39;#2019-ncov&#39;, &#39;pandemic&#39;, &#39;#pandemic #2019ncov&#39;, &#39;2019ncov&#39;, &#39;quarantine&#39;, &#39;#quarantine&#39;, &#39;flatten the curve&#39;, &#39;flattening the curve&#39;, &#39;#flatteningthecurve&#39;, &#39;#flattenthecurve&#39;, &#39;hand sanitizer&#39;, &#39;#handsanitizer&#39;, &#39;#lockdown&#39;, &#39;lockdown&#39;, &#39;social distancing&#39;, &#39;#socialdistancing&#39;, &#39;work from home&#39;, &#39;#workfromhome&#39;, &#39;working from home&#39;, &#39;#workingfromhome&#39;, &#39;ppe&#39;, &#39;n95&#39;, &#39;#ppe&#39;, &#39;#n95&#39;, &#39;#covidiots&#39;, &#39;covidiots&#39;, &#39;herd immunity&#39;, &#39;#herdimmunity&#39;, &#39;pneumonia&#39;, &#39;#pneumonia&#39;, &#39;chinese virus&#39;, &#39;#chinesevirus&#39;, &#39;wuhan virus&#39;, &#39;#wuhanvirus&#39;, &#39;kung flu&#39;, &#39;#kungflu&#39;, &#39;wearamask&#39;, &#39;#wearamask&#39;, &#39;wear a mask&#39;, &#39;vaccine&#39;, &#39;vaccines&#39;, &#39;#vaccine&#39;, &#39;#vaccines&#39;, &#39;corona vaccine&#39;, &#39;corona vaccines&#39;, &#39;#coronavaccine&#39;, &#39;#coronavaccines&#39;, &#39;face shield&#39;, &#39;#faceshield&#39;, &#39;face shields&#39;, &#39;#faceshields&#39;, &#39;health worker&#39;, &#39;#healthworker&#39;, &#39;health workers&#39;, &#39;#healthworkers&#39;, &#39;#stayhomestaysafe&#39;, &#39;#coronaupdate&#39;, &#39;#frontlineheroes&#39;, &#39;#coronawarriors&#39;, &#39;#homeschool&#39;, &#39;#homeschooling&#39;, &#39;#hometasking&#39;, &#39;#masks4all&#39;, &#39;#wfh&#39;, &#39;wash ur hands&#39;, &#39;wash your hands&#39;, &#39;#washurhands&#39;, &#39;#washyourhands&#39;, &#39;#stayathome&#39;, &#39;#stayhome&#39;, &#39;#selfisolating&#39;, &#39;self isolating&#39;) Getting # followers names &lt;- c(&#39;BarackObama&#39;, &#39;justinbieber&#39;, &#39;ArianaGrande&#39;) temp &lt;- data.frame() for (name in names){ line &lt;- get_timelines(name, n=20) temp &lt;- rbind(line, temp) } followers_count &lt;- temp$followers_count temp ## # A tibble: 60 x 90 ## user_id status_id created_at screen_name text source ## &lt;chr&gt; &lt;chr&gt; &lt;dttm&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 34507480 1465479935712137217 2021-11-30 00:36:58 ArianaGrande &quot;!!!!! ~ Twitt~ ## 2 34507480 1465472555527991299 2021-11-30 00:07:39 ArianaGrande &quot;one ho~ Twitt~ ## 3 34507480 1465458087087656963 2021-11-29 23:10:09 ArianaGrande &quot;https:~ Twitt~ ## 4 34507480 1465456598982164480 2021-11-29 23:04:14 ArianaGrande &quot;two ho~ Twitt~ ## 5 34507480 1465447373258166272 2021-11-29 22:27:35 ArianaGrande &quot;on my ~ Twitt~ ## 6 34507480 1465443184461299717 2021-11-29 22:10:56 ArianaGrande &quot;The #V~ Twitt~ ## 7 34507480 1465417007197605888 2021-11-29 20:26:55 ArianaGrande &quot;jimmy ~ Twitt~ ## 8 34507480 1465388892660453376 2021-11-29 18:35:12 ArianaGrande &quot;every ~ Twitt~ ## 9 34507480 1465381395983790088 2021-11-29 18:05:24 ArianaGrande &quot;foreve~ Twitt~ ## 10 34507480 1465378351590219779 2021-11-29 17:53:19 ArianaGrande &quot;@vcutg~ Twitt~ ## # ... with 50 more rows, and 84 more variables: display_text_width &lt;dbl&gt;, ## # reply_to_status_id &lt;chr&gt;, reply_to_user_id &lt;chr&gt;, ## # reply_to_screen_name &lt;chr&gt;, is_quote &lt;lgl&gt;, is_retweet &lt;lgl&gt;, ## # favorite_count &lt;int&gt;, retweet_count &lt;int&gt;, quote_count &lt;int&gt;, ## # reply_count &lt;int&gt;, hashtags &lt;list&gt;, symbols &lt;list&gt;, urls_url &lt;list&gt;, ## # urls_t.co &lt;list&gt;, urls_expanded_url &lt;list&gt;, media_url &lt;list&gt;, ## # media_t.co &lt;list&gt;, media_expanded_url &lt;list&gt;, media_type &lt;list&gt;, ... Reference: https://stackoverflow.com/questions/31348453/how-do-i-clean-twitter-data-in-r library(stringr) clean_tweets &lt;- function(x) { x %&gt;% # Remove URLs str_remove_all(&quot; ?(f|ht)(tp)(s?)(://)(.*)[.|/](.*)&quot;) %&gt;% # Remove mentions e.g. &quot;@my_account&quot; str_remove_all(&quot;@[[:alnum:]_]{4,}&quot;) %&gt;% # Replace &quot;&amp;&quot; character reference with &quot;and&quot; str_replace_all(&quot;&amp;amp;&quot;, &quot;and&quot;) %&gt;% # Remove punctuation, using a standard character class str_remove_all(&quot;[[:punct:]]&quot;) %&gt;% # Remove &quot;RT: &quot; from beginning of retweets str_remove_all(&quot;^RT:? &quot;) %&gt;% # Replace any newline characters with a space str_replace_all(&quot;\\\\\\n&quot;, &quot; &quot;) %&gt;% # Make everything lowercase str_to_lower() %&gt;% # Remove any trailing whitespace around the text str_trim(&quot;both&quot;) %&gt;% # remove unnecessary space str_replace_all(&quot; &quot;,&quot; &quot;) } Clean the tweet data and extract the tweets that contains COVID related tags output &lt;- c() cleaned_text &lt;- clean_tweets(temp$text) for (words in str_split(cleaned_text, &quot; &quot;)) { f &lt;- if_else(any(words %in% tag), 1, 0) output &lt;- append(output, f) } temp$output &lt;- output temp %&gt;% filter(output == 1) -&gt; temp2 temp2 &lt;- temp2 %&gt;% select(user_id, text, lang, name, location, followers_count) temp2 ## # A tibble: 1 x 6 ## user_id text lang name location followers_count ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; ## 1 813286 The epidemic of gun violence~ en Barack ~ Washingt~ 130310207 temp2[, c(&quot;joy&quot;, &quot;trust&quot;, &quot;anticipation&quot;, &quot;sadness&quot;, &quot;fear&quot;, &quot;anger&quot;, &quot;surprise&quot;, &quot;disgust&quot;, &quot;sentiment_score&quot;)] &lt;- NA temp2 ## # A tibble: 1 x 15 ## user_id text lang name location followers_count joy trust anticipation ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;lgl&gt; ## 1 813286 The epi~ en Bara~ Washing~ 130310207 NA NA NA ## # ... with 6 more variables: sadness &lt;lgl&gt;, fear &lt;lgl&gt;, anger &lt;lgl&gt;, ## # surprise &lt;lgl&gt;, disgust &lt;lgl&gt;, sentiment_score &lt;lgl&gt; library(dplyr) library(tidytext) library(textdata) for(i in 1:nrow(temp2)) { tweets_data &lt;- temp2[i,] %&gt;% unnest_tokens(word, text) %&gt;% # break down into individual words filter(!nchar(word) &lt; 3) %&gt;% # remove words that are unnecessary, like exclamations anti_join(stop_words) # need to remove stop words because they are also unnecessary tweets_nrc &lt;- tweets_data %&gt;% inner_join(get_sentiments(&quot;nrc&quot;)) %&gt;% filter(!sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;)) tweets_nrc_np &lt;- tweets_data %&gt;% inner_join(get_sentiments(&quot;nrc&quot;)) %&gt;% filter(sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;)) %&gt;% mutate(score = if_else(sentiment==&quot;positive&quot;, 1, -1)) sentiment_score &lt;- sum(tweets_nrc_np$score) / nrow(tweets_nrc_np) sentiments &lt;- tweets_nrc %&gt;% group_by(sentiment, .drop = FALSE) %&gt;% summarise(word_count = n()) %&gt;% ungroup() if(&quot;joy&quot; %in% sentiments$sentiment){ temp2[i, &quot;joy&quot;] &lt;- sentiments$word_count[which(sentiments$sentiment==&quot;joy&quot;)] } if(&quot;trust&quot; %in% sentiments$sentiment){ temp2[i, &quot;trust&quot;] &lt;- sentiments$word_count[which(sentiments$sentiment==&quot;trust&quot;)] } if(&quot;anticipation&quot; %in% sentiments$sentiment){ temp2[i, &quot;anticipation&quot;] &lt;- sentiments$word_count[which(sentiments$sentiment==&quot;anticipation&quot;)] } if(&quot;sadness&quot; %in% sentiments$sentiment){ temp2[i, &quot;sadness&quot;] &lt;- sentiments$word_count[which(sentiments$sentiment==&quot;sadness&quot;)] } if(&quot;fear&quot; %in% sentiments$sentiment){ temp2[i, &quot;fear&quot;] &lt;- sentiments$word_count[which(sentiments$sentiment==&quot;fear&quot;)] } if(&quot;anger&quot; %in% sentiments$sentiment){ temp2[i, &quot;anger&quot;] &lt;- sentiments$word_count[which(sentiments$sentiment==&quot;anger&quot;)] } if(&quot;surprise&quot; %in% sentiments$sentiment){ temp2[i, &quot;surprise&quot;] &lt;- sentiments$word_count[which(sentiments$sentiment==&quot;surprise&quot;)] } if(&quot;disgust&quot; %in% sentiments$sentiment){ temp2[i, &quot;disgust&quot;] &lt;- sentiments$word_count[which(sentiments$sentiment==&quot;disgust&quot;)] } # temp2[i,&quot;joy&quot;] &lt;- if_else(&quot;joy&quot; %in% sentiments$sentiment, sentiments$word_count[which(sentiments$sentiment==&quot;joy&quot;)], 0) # temp2[i,&quot;trust&quot;] &lt;- if_else(&quot;trust&quot; %in% sentiments$sentiment, sentiments$word_count[which(sentiments$sentiment==&quot;trust&quot;)], 0) # temp2[i,&quot;anticipation&quot;] &lt;- if_else(&quot;anticipation&quot; %in% sentiments$sentiment, sentiments$word_count[which(sentiments$sentiment==&quot;anticipation&quot;)], 0) # temp2[i,&quot;sadness&quot;] &lt;- if_else(&quot;sadness&quot; %in% sentiments$sentiment, sentiments$word_count[which(sentiments$sentiment==&quot;sadness&quot;)], 0) # temp2[i,&quot;fear&quot;] &lt;- if_else(&quot;fear&quot; %in% sentiments$sentiment, sentiments$word_count[which(sentiments$sentiment==&quot;fear&quot;)], 0) # temp2[i,&quot;anger&quot;] &lt;- if_else(&quot;anger&quot; %in% sentiments$sentiment, sentiments$word_count[which(sentiments$sentiment==&quot;anger&quot;)], 0) # temp2[i,&quot;surprise&quot;] &lt;- if_else(&quot;surprise&quot; %in% sentiments$sentiment, sentiments$word_count[which(sentiments$sentiment==&quot;surprise&quot;)], 0) # temp2[i,&quot;disgust&quot;] &lt;- if_else(&quot;disgust&quot; %in% sentiments$sentiment, sentiments$word_count[which(sentiments$sentiment==&quot;disgust&quot;)], 0) temp2[i, &quot;sentiment_score&quot;] &lt;- sentiment_score } temp2 ## # A tibble: 1 x 15 ## user_id text lang name location followers_count joy trust anticipation ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;lgl&gt; &lt;int&gt; &lt;int&gt; ## 1 813286 The epi~ en Bara~ Washing~ 130310207 NA 1 1 ## # ... with 6 more variables: sadness &lt;int&gt;, fear &lt;int&gt;, anger &lt;int&gt;, ## # surprise &lt;int&gt;, disgust &lt;int&gt;, sentiment_score &lt;dbl&gt; tweets_nrc &lt;- tweets_data %&gt;% inner_join(get_sentiments(&quot;nrc&quot;)) %&gt;% filter(!sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;)) %&gt;% select(user_id, lang, name, location, followers_count, word, sentiment) tweets_nrc_np &lt;- tweets_data %&gt;% inner_join(get_sentiments(&quot;nrc&quot;)) %&gt;% filter(sentiment %in% c(&quot;positive&quot;, &quot;negative&quot;)) %&gt;% select(user_id, lang, name, location, followers_count, word, sentiment) %&gt;% mutate(score = if_else(sentiment==&quot;positive&quot;, 1, -1)) tweets_nrc_np ## # A tibble: 8 x 8 ## user_id lang name location followers_count word sentiment score ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 813286 en Barack Obama Washington,~ 130310207 epide~ negative -1 ## 2 813286 en Barack Obama Washington,~ 130310207 gun negative -1 ## 3 813286 en Barack Obama Washington,~ 130310207 viole~ negative -1 ## 4 813286 en Barack Obama Washington,~ 130310207 worse negative -1 ## 5 813286 en Barack Obama Washington,~ 130310207 pande~ negative -1 ## 6 813286 en Barack Obama Washington,~ 130310207 bias negative -1 ## 7 813286 en Barack Obama Washington,~ 130310207 reform positive 1 ## 8 813286 en Barack Obama Washington,~ 130310207 police positive 1 Barplot for sentiments (excluding positive and negative) library(ggplot2) sentiment_plot &lt;- tweets_nrc %&gt;% group_by(sentiment) %&gt;% summarise(word_count = n()) %&gt;% ungroup() %&gt;% mutate(sentiment = reorder(sentiment, word_count)) %&gt;% #Use `fill = -word_count` to make the larger bars darker ggplot(aes(sentiment, word_count, fill = -word_count)) + geom_col() + guides(fill = &quot;none&quot;) + # no legend labs(x = NULL, y = &quot;Word Count&quot;) + # scale_y_continuous(limits = c(0, 100)) + ggtitle(&quot;Tweeter Sentiment&quot;) + coord_flip() sentiment_plot Comparison of positive vs. negative sentiment_plot_np &lt;- tweets_nrc_np %&gt;% group_by(sentiment) %&gt;% summarise(word_count = n()) %&gt;% ungroup() %&gt;% mutate(sentiment = reorder(sentiment, word_count)) %&gt;% #Use `fill = -word_count` to make the larger bars darker ggplot(aes(sentiment, word_count, fill = -word_count)) + geom_col() + guides(fill = &quot;none&quot;) + # no legend labs(x = NULL, y = &quot;Word Count&quot;) + # scale_y_continuous(limits = c(0, 100)) + ggtitle(&quot;Tweeter Sentiment&quot;) + coord_flip() sentiment_score &lt;- sum(tweets_nrc_np$score) / nrow(tweets_nrc_np) sentiment_score ## [1] -0.5 sentiment_plot_np "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
